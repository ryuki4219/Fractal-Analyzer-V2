"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«è§£æãƒ„ãƒ¼ãƒ«
trained_fd_model.pkl ã®å†…å®¹ã‚’è©³ã—ãè§£æã—ã¾ã™
"""

import pickle
import os

def analyze_model(model_path):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
    """
    print("=" * 80)
    print("ğŸ” å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«è§£æãƒ„ãƒ¼ãƒ«")
    print("=" * 80)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
    file_size = os.path.getsize(model_path)
    print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:")
    print(f"  - ãƒ‘ã‚¹: {model_path}")
    print(f"  - ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024:.2f} KB)")
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        print(f"  - âœ… èª­ã¿è¾¼ã¿æˆåŠŸ")
    except Exception as e:
        print(f"  - âŒ èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return
    
    # ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬æƒ…å ±
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«åŸºæœ¬æƒ…å ±:")
    print(f"  - å‹: {type(model).__name__}")
    print(f"  - ã‚¯ãƒ©ã‚¹: {model.__class__.__module__}.{model.__class__.__name__}")
    
    # LightGBMãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±
    if hasattr(model, 'n_estimators'):
        print(f"\nğŸ“Š LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  - æ¨å®šå™¨æ•° (n_estimators): {model.n_estimators}")
        print(f"  - æœ€å¤§æ·±åº¦ (max_depth): {model.max_depth}")
        print(f"  - å­¦ç¿’ç‡ (learning_rate): {model.learning_rate}")
        print(f"  - ä¸¦åˆ—æ•° (n_jobs): {model.n_jobs}")
    
    # è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±
    if hasattr(model, 'booster_'):
        print(f"\nğŸŒ³ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        booster = model.booster_
        
        # ç‰¹å¾´é‡ã®æ•°
        if hasattr(model, 'n_features_in_'):
            print(f"  - å…¥åŠ›ç‰¹å¾´é‡æ•°: {model.n_features_in_}")
        
        # æœ¨ã®æ•°
        num_trees = model.n_estimators
        print(f"  - æ±ºå®šæœ¨ã®æ•°: {num_trees}")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ãƒ³ãƒ—ï¼ˆæœ€åˆã®æœ¨ã®ã¿ï¼‰
        try:
            model_dump = booster.dump_model()
            print(f"\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«æ§‹é€ æƒ…å ±:")
            print(f"  - ç‰¹å¾´é‡å: {model_dump.get('feature_names', 'N/A')}")
            print(f"  - æœ¨ã®æ•°: {len(model_dump.get('tree_info', []))}")
            
            # æœ€åˆã®æœ¨ã®æƒ…å ±
            if model_dump.get('tree_info'):
                first_tree = model_dump['tree_info'][0]
                print(f"\nğŸŒ² æœ€åˆã®æ±ºå®šæœ¨ã®è©³ç´°:")
                print(f"  - æœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {first_tree.get('tree_index', 'N/A')}")
                print(f"  - ãƒãƒ¼ãƒ‰æ•°: {first_tree.get('num_leaves', 'N/A')}")
        except Exception as e:
            print(f"  - ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å–å¾—ã«å¤±æ•—: {e}")
    
    # ç‰¹å¾´é‡ã®é‡è¦åº¦
    if hasattr(model, 'feature_importances_'):
        print(f"\nâ­ ç‰¹å¾´é‡ã®é‡è¦åº¦:")
        importances = model.feature_importances_
        
        # ç‰¹å¾´é‡åï¼ˆã“ã®ã‚¢ãƒ—ãƒªã®å ´åˆï¼‰
        feature_names = [
            "å¹³å‡å€¤ (mean)",
            "æ¨™æº–åå·® (std)",
            "ã‚¨ãƒƒã‚¸å¼·åº¦ (edge_strength)",
            "ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« (noise_level)",
            "ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (entropy)"
        ]
        
        for i, (name, importance) in enumerate(zip(feature_names, importances)):
            bar_length = int(importance / max(importances) * 40)
            bar = "â–ˆ" * bar_length
            print(f"  [{i+1}] {name:30s}: {importance:8.1f} {bar}")
        
        # æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡
        most_important_idx = importances.argmax()
        print(f"\n  ğŸ† æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡: {feature_names[most_important_idx]}")
    
    # äºˆæ¸¬é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ§ª äºˆæ¸¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ:")
    try:
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        import numpy as np
        test_input = np.array([[0.5, 0.1, 0.3, 0.05, 6.5]])  # 5æ¬¡å…ƒç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        prediction = model.predict(test_input)
        print(f"  - âœ… äºˆæ¸¬æˆåŠŸ")
        print(f"  - ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›: {test_input[0]}")
        print(f"  - äºˆæ¸¬å‡ºåŠ› (FD): {prediction[0]:.4f}")
    except Exception as e:
        print(f"  - âŒ äºˆæ¸¬å¤±æ•—: {e}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ç”¨é€”èª¬æ˜
    print(f"\nğŸ’¡ ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ç”¨é€”:")
    print(f"""
  ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒäºˆæ¸¬AIã€ã§ã™ã€‚
  
  ã€å…¥åŠ›ã€‘: ä½ç”»è³ªç”»åƒã®5ã¤ã®ç‰¹å¾´é‡
    1. å¹³å‡å€¤ (ç”»åƒã®æ˜ã‚‹ã•)
    2. æ¨™æº–åå·® (æ˜ã‚‹ã•ã®ã°ã‚‰ã¤ã)
    3. ã‚¨ãƒƒã‚¸å¼·åº¦ (è¼ªéƒ­ã®é®®æ˜ã•)
    4. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« (ç”»åƒã®ãƒã‚¤ã‚ºé‡)
    5. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (æƒ…å ±é‡)
  
  ã€å‡ºåŠ›ã€‘: é«˜ç”»è³ªç›¸å½“ã®ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ (1.0ã€œ2.0ç¨‹åº¦)
  
  ã€ä½¿ã„æ–¹ã€‘:
    - æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã§ä½ç”»è³ªç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    - ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒè‡ªå‹•çš„ã«é«˜å“è³ªFDã‚’äºˆæ¸¬
    - é«˜ç”»è³ªç”»åƒãŒãªãã¦ã‚‚æ­£ç¢ºãªå€¤ã‚’å–å¾—å¯èƒ½
  """)
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹æ¨å®š
    print(f"\nğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ¨å®š:")
    if hasattr(model, 'n_features_in_'):
        print(f"  - ç‰¹å¾´é‡æ•°: {model.n_features_in_}æ¬¡å…ƒ")
    if hasattr(model, 'n_estimators'):
        print(f"  - ä½¿ç”¨ã—ãŸæ±ºå®šæœ¨: {model.n_estimators}å€‹")
        print(f"  - æ¨å®šå­¦ç¿’æ™‚é–“: {model.n_estimators * 0.01:.2f}ç§’ (æ¦‚ç®—)")
    
    print("\n" + "=" * 80)
    print("âœ… è§£æå®Œäº†")
    print("=" * 80)
    
    return model


if __name__ == "__main__":
    # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
    model_path = r"C:\Users\iikrk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\trained_fd_model (1).pkl"
    
    # ã¾ãŸã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹å ´åˆ
    if not os.path.exists(model_path):
        model_path = "trained_fd_model.pkl"
    
    model = analyze_model(model_path)
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ‡ãƒ«ã‚’ã•ã‚‰ã«è©³ã—ãèª¿ã¹ãŸã„å ´åˆ
    if model is not None:
        print(f"\nğŸ’¬ è¿½åŠ æƒ…å ±ãŒå¿…è¦ãªå ´åˆ:")
        print(f"  - ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: å¤‰æ•° 'model' ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
        print(f"  - å±æ€§ä¸€è¦§: dir(model)")
        print(f"  - ãƒ˜ãƒ«ãƒ—: help(model)")
